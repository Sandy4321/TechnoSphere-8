{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Алгоритмы интеллектуальной обработки больших объемов данных\n",
    "## Домашнее задание №2 - Дерево решений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Общая информация**\n",
    "\n",
    "**Срок сдачи:** до 27 марта 2018, 06:00   \n",
    "**Штраф за опоздание:** -2 балла после 06:00 27 марта, -4 балла после 06:00 3 апреля, -6 баллов после 06:00 10 апреля\n",
    "\n",
    "При отправлении ДЗ указывайте фамилию в названии файла   \n",
    "\n",
    "\n",
    "Присылать ДЗ необходимо в виде ссылки на свой github репозиторий в slack @alkhamush\n",
    "Необходимо в slack создать таск в приватный чат:   \n",
    "/todo Фамилия Имя *ссылка на гитхаб* @alkhamush   \n",
    "Пример:   \n",
    "/todo Ксения Стройкова https://github.com/stroykova/spheremailru/stroykova_hw2.ipynb @alkhamush   \n",
    "\n",
    "Используйте данный Ipython Notebook при оформлении домашнего задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Задание 1 (2 баллов)\n",
    "Разберитесь в коде MyDecisionTreeClassifier, который уже частично реализован. В комментариях, где написано \"Что делает этот блок кода?\", ответьте на этот вопрос. Допишите код там, где написано \"Ваш код\". Ваша реализация дерева должна работать по точности не хуже DecisionTreeClassifier из sklearn. Точность проверяется на wine и Speed Dating Data.\n",
    "\n",
    "###### Задание 2 (2 балла)\n",
    "Добиться скорости работы на fit сравнимой со sklearn wine и Speed Dating Data. \n",
    "Для этого используем numpy. \n",
    "\n",
    "###### Задание 3 (2 балла)\n",
    "Продемонстрируйте умение работать с Pipeline на данных Speed Dating Data и DecisionTreeClassifier. Нужно в pipeline произвести все необходимые преобразования данных и в конце обучить модель. Задание реализуйте под пунктом Задание 3 (уже написано ниже)\n",
    "\n",
    "###### Задание 4 (2 балла)\n",
    "Добавьте функционал, который определяет значения feature importance. Выведите 10 главных фичей под пунктом Задание 4 (уже написано ниже) для MyDecisionTreeClassifier и DecisionTreeClassifier так, чтобы сразу были видны выводы и по MyDecisionTreeClassifier, и по DecisionTreeClassifier. Используем данные Speed Dating Data.\n",
    "\n",
    "###### Задание 5 (2 балла)\n",
    "С помощью GridSearchCV или RandomSearchCV подберите наиболее оптимальные параметры для случайного леса (Выберете 2-3 параметра). Используем данные Speed Dating Data. Задание реализуйте под пунктом Задание 5 (уже написано ниже)\n",
    "\n",
    "\n",
    "**Штрафные баллы:**\n",
    "\n",
    "1. Невыполнение PEP8 -1 балл\n",
    "2. Отсутствие фамилии в имени скрипта (скрипт должен называться по аналогии со stroykova_hw2.ipynb) -1 балл\n",
    "3. Все строчки должны быть выполнены. Нужно, чтобы output команды можно было увидеть уже в git'е. В противном случае -1 балл\n",
    "4. При оформлении ДЗ нужно пользоваться данным файлом в качестве шаблона. Не нужно удалять и видоизменять написанный код и текст. В противном случае -1 балл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "#%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "\n",
    "class MyDecisionTreeClassifier:\n",
    "    NON_LEAF_TYPE = 0\n",
    "    LEAF_TYPE = 1\n",
    "\n",
    "    def __init__(self, min_samples_split=2, max_depth=None, sufficient_share=1.0,\n",
    "                 criterion='gini', max_features=None):\n",
    "        self.tree = dict()\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        self.sufficient_share = sufficient_share\n",
    "        self.num_class = -1\n",
    "        self.feature_importances_ = None\n",
    "        if criterion == 'gini':\n",
    "            self.G_function = self.__gini\n",
    "        elif criterion == 'entropy':\n",
    "            self.G_function = self.__entropy\n",
    "        elif criterion == 'misclass':\n",
    "            self.G_function = self.__misclass\n",
    "        else:\n",
    "            print('invalid criterion name')\n",
    "            raise\n",
    "\n",
    "        if max_features == 'sqrt':\n",
    "            self.get_feature_ids = self.__get_feature_ids_sqrt\n",
    "        elif max_features == 'log2':\n",
    "            self.get_feature_ids = self.__get_feature_ids_log2\n",
    "        elif max_features == None:\n",
    "            self.get_feature_ids = self.__get_feature_ids_N\n",
    "        else:\n",
    "            print('invalid max_features name')\n",
    "            raise\n",
    "\n",
    "    def __gini(self, l_c, l_s, r_c, r_s):\n",
    "        l_s = l_s.astype('float')\n",
    "        r_s = r_s.astype('float')\n",
    "        return 1 - (np.sum(l_c ** 2 / l_s, axis=1) + \\\n",
    "                    np.sum(r_c ** 2 / r_s, axis=1)) / ((l_s + r_s)[0])\n",
    "\n",
    "    def __entropy(self, l_c, l_s, r_c, r_s):\n",
    "        return -(np.nansum(np.log(l_c / l_s) * l_c, axis=1) + \\\n",
    "                 np.nansum(np.log(r_c / r_s) * r_c, axis=1)) / ((l_s + r_s)[0])\n",
    "\n",
    "    def __misclass(self, l_c, l_s, r_c, r_s):\n",
    "        return 1 - (np.max(l_c, axis=1) + np.max(r_c, axis=1)) / ((l_s + r_s)[0])\n",
    "\n",
    "    def __get_feature_ids_sqrt(self, n_feature):\n",
    "        feature_ids = range(n_feature)\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return np.array(feature_ids[:int(np.sqrt(n_feature))])\n",
    "\n",
    "    def __get_feature_ids_log2(self, n_feature):\n",
    "        feature_ids = range(n_feature)\n",
    "        np.random.shuffle(feature_ids)\n",
    "        return np.array(feature_ids[:int(np.log2(n_feature))])\n",
    "\n",
    "    def __get_feature_ids_N(self, n_feature):\n",
    "        return np.arange(n_feature)\n",
    "\n",
    "    def __sort_samples(self, x, y):\n",
    "        sorted_idx = x.argsort()\n",
    "        return x[sorted_idx], y[sorted_idx]\n",
    "\n",
    "    def __div_samples(self, x, y, feature_id, threshold):\n",
    "        left_mask = x[:, feature_id] > threshold\n",
    "        right_mask = ~left_mask\n",
    "        return x[left_mask], x[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "    def __find_threshold(self, x, y):\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        sorted_x, sorted_y = self.__sort_samples(x, y)  # получили отсортированный по возрастанию\n",
    "        #x и соответсвующий y\n",
    "        # class_number = np.unique(y).shape[0] #сколько разных классов есть в данном y\n",
    "        # но это не сработает, когда при очередном разбиении выбыл какой-то непоследний класс.\n",
    "        #Сработает для бинарной.\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        splitted_sorted_y = sorted_y[self.min_samples_split:-self.min_samples_split]\n",
    "        # берем все, кроме первых и последних min_sample_split элементов, обеспечивая то,\n",
    "        #что в вершине будет минимальное\n",
    "        # заданное число вершин\n",
    "        r_border_ids = np.where(splitted_sorted_y[:-1] != splitted_sorted_y[1:])[0] \\\n",
    "        + (self.min_samples_split + 1)\n",
    "        # r_border_ids те индексы, когда текущий элемент не равен следующему\n",
    "        #сдвинутые вправо на (self.min_samples_split + 1)\n",
    "        # то есть по факту границы элементов, когда класс поменялся\n",
    "\n",
    "        if len(r_border_ids) == 0:\n",
    "            return float('+inf'), None\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        eq_el_count = r_border_ids - np.append([self.min_samples_split], r_border_ids[:-1])\n",
    "        # eq_el_count - массив, каждый элемент которого - количество подряд идущих одинаковых элементов\n",
    "        one_hot_code = np.zeros((r_border_ids.shape[0], self.num_class))\n",
    "        one_hot_code[np.arange(r_border_ids.shape[0]), sorted_y[r_border_ids - 1]] = 1\n",
    "        # one_hot_code поставили 1 в тех местах, в которых начинается новый класс\n",
    "        class_increments = one_hot_code * eq_el_count.reshape(-1, 1)\n",
    "        # class_increments - матрица того же размера что и one_hot_code, и теперь вместо 1 на тех же местах\n",
    "        # там будут стоять количества элементов, которые шли подряд\n",
    "        class_increments[0] = class_increments[0] + np.bincount(sorted_y[:self.min_samples_split],\n",
    "                                                                minlength=self.num_class)\n",
    "        # class_increments[0] дописывает в первую строчку ещё и те количества элементов которые шли подряд\n",
    "        # которые мы не учитывали при min_sample_split\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        l_class_count = np.cumsum(class_increments, axis=0)\n",
    "        # l_class_count для удобства, чтобы сразу понять, какие элементы пойдут налево при разбиении\n",
    "        # по на данном индексе\n",
    "        r_class_count = np.bincount(sorted_y, minlength=self.num_class) - l_class_count\n",
    "        # а это элементы, которое пойдет вправо\n",
    "        l_sizes = r_border_ids.reshape(l_class_count.shape[0], 1)\n",
    "        # l_sizes количество элементов, которое пойдет влево\n",
    "        r_sizes = sorted_y.shape[0] - l_sizes\n",
    "        # r_sizes количество элементов, которое пойдет вправо\n",
    "\n",
    "        # Что делает этот блок кода?\n",
    "        # здесь мы высчитываем наименьший impurity\n",
    "        gs = self.G_function(l_class_count, l_sizes, r_class_count, r_sizes)\n",
    "        idx = np.argmin(gs)\n",
    "\n",
    "        # с left_el_id начинается правая часть выборки\n",
    "        # возвращаем минимальное значение gs и порог по признаку\n",
    "        left_el_id = l_sizes[idx][0]\n",
    "        return gs[idx], (sorted_x[left_el_id - 1] + sorted_x[left_el_id]) / 2.0\n",
    "\n",
    "    def __fit_node(self, x, y, node_id, depth, pred_f=-1):\n",
    "\n",
    "        # создаем листовые вершины\n",
    "        # если сейчас в узле объектов меньше чем в min_samples_split\n",
    "        if (x.shape[0] <= self.min_samples_split):\n",
    "            self.__create_leaf(y, node_id, depth)\n",
    "            return\n",
    "        elif (depth == self.max_depth):  # если достигнута максимальная глубина\n",
    "            self.__create_leaf(y, node_id, depth)\n",
    "            return\n",
    "        elif np.any(np.unique(y, return_counts=True)[1] / y.shape[0] >= self.sufficient_share):\n",
    "            # если какого то класса существенно больше\n",
    "            self.__create_leaf(y, node_id, depth)\n",
    "            return\n",
    "        elif (np.unique(y).shape[0] == 1):\n",
    "            # если уже только один класс\n",
    "            self.__create_leaf(y, node_id, depth)\n",
    "            return\n",
    "\n",
    "        # это массивы для порогов, impurity\n",
    "        f_size = x.shape[1]\n",
    "        threshold = np.zeros(f_size)\n",
    "        impurity = np.zeros(f_size)\n",
    "        f_to_fit = self.get_feature_ids(f_size)\n",
    "        for i in f_to_fit:\n",
    "            impurity[i], threshold[i] = self.__find_threshold(x[:, i], y)\n",
    "\n",
    "        # это пригодится для определения feature importance\n",
    "        if self.G_function == self.__gini:\n",
    "            my_purity = 1 - np.sum((np.unique(y, return_counts=True)[1] * 1. / y.shape[0]) ** 2)\n",
    "        elif self.G_function == self.__misclass:\n",
    "            my_purity = 1 - np.max(np.unique(y, return_counts=True)[1] * 1. / y.shape[0])\n",
    "        elif self.G_function == self.__entropy:\n",
    "            my_purity = -(np.nansum(\n",
    "                np.log(np.unique(y, return_counts=True)[1] * 1. / y.shape[0]) \\\n",
    "                * np.unique(y, return_counts=True)[1] * 1. / y.shape[0], axis=1))\n",
    "\n",
    "        # находим разбиение, соответствующее минимальному impurity\n",
    "        f_min = impurity.argmin()\n",
    "\n",
    "        # если разбить не удалось, создаем лист\n",
    "        if np.isnan(threshold[f_min]):\n",
    "            self.__create_leaf(y, node_id, depth)\n",
    "            return\n",
    "\n",
    "        # получаем разбиение\n",
    "        x_left, x_right, y_left, y_right = self.__div_samples(x, y, f_min, threshold[f_min])\n",
    "\n",
    "        # если что-то имеет размер 0, то создаем лист\n",
    "        if (x_left.shape[0] == 0 or x_right.shape[0] == 0):\n",
    "            self.__create_leaf(y, node_id, depth)\n",
    "            return\n",
    "\n",
    "        # иначе создаем узел, в узле хранится тип вершины, индекс на котором произошло разбиение. \n",
    "        # порог, размер текущей выборки, impurity, и собственная I(S)\n",
    "        self.tree[node_id] = (\n",
    "        self.NON_LEAF_TYPE, f_min, threshold[f_min], x.shape[0], x.shape[1], impurity[f_min], my_purity)\n",
    "        self.__fit_node(x_left, y_left, 2 * node_id + 1, depth + 1)\n",
    "        self.__fit_node(x_right, y_right, 2 * node_id + 2, depth + 1)\n",
    "\n",
    "        # функция создания листа\n",
    "\n",
    "    def __create_leaf(self, y, node_id, depth):\n",
    "        counts = np.bincount(y, minlength=self.num_class)\n",
    "        probs = counts.astype(float) / y.shape[0]\n",
    "        self.tree[node_id] = (self.LEAF_TYPE, np.argmax(counts), probs, -1, -1, -1, -1)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.num_class = np.unique(y).size\n",
    "        self.__fit_node(x, y, 0, 0)\n",
    "\n",
    "    def __predict_class(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold, _, _, _, _ = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_class(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_class(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[1]\n",
    "\n",
    "    def __predict_probs(self, x, node_id):\n",
    "        node = self.tree[node_id]\n",
    "        if node[0] == self.__class__.NON_LEAF_TYPE:\n",
    "            _, feature_id, threshold, _, _, _, _ = node\n",
    "            if x[feature_id] > threshold:\n",
    "                return self.__predict_probs(x, 2 * node_id + 1)\n",
    "            else:\n",
    "                return self.__predict_probs(x, 2 * node_id + 2)\n",
    "        else:\n",
    "            return node[2]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.__predict_class(x, 0) for x in X])\n",
    "\n",
    "    def predict_probs(self, X):\n",
    "        return np.array([self.__predict_probs(x, 0) for x in X])\n",
    "\n",
    "    def fit_predict(self, x_train, y_train, predicted_x):\n",
    "        self.fit(x_train, y_train)\n",
    "        return self.predict(predicted_x)\n",
    "\n",
    "    def feature_importances(self):\n",
    "        imp = np.zeros(self.tree.values()[0][4])  # imp-importance, имеет размер = количеству признаков\n",
    "        internal = sum([(1 - v[0]) for v in self.tree.itervalues()])  # считаем число узловых вершин\n",
    "        for k, v in self.tree.items():\n",
    "            if not v[0]:\n",
    "                imp[v[1]] += (v[6] + v[5]) / internal  # действием в соответствии с формулой из лекции\n",
    "        norm = np.sum(imp)  # перенормировка\n",
    "        return imp * 1. / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.1, stratify=wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.52 ms, sys: 0 ns, total: 2.52 ms\n",
      "Wall time: 2.98 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.44 ms, sys: 3.4 ms, total: 11.8 ms\n",
      "Wall time: 15 ms\n"
     ]
    }
   ],
   "source": [
    "%time my_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества работы на wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9440559440559441"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Speed Dating Data.csv',\n",
    "                 encoding='cp1251')\n",
    "df = df.iloc[:, :97]\n",
    "df = df.drop(['id'], axis=1)\n",
    "df = df.drop(['idg'], axis=1)\n",
    "df = df.drop(['condtn'], axis=1)\n",
    "df = df.drop(['round'], axis=1)\n",
    "df = df.drop(['position', 'positin1'], axis=1)\n",
    "df = df.drop(['order'], axis=1)\n",
    "df = df.drop(['partner'], axis=1)\n",
    "df = df.drop(['age_o', 'race_o', 'pf_o_att',\n",
    "              'pf_o_sin', 'pf_o_int',\n",
    "              'pf_o_fun', 'pf_o_amb', 'pf_o_sha',\n",
    "              'dec_o', 'attr_o', 'sinc_o', 'intel_o', 'fun_o',\n",
    "              'amb_o', 'shar_o', 'like_o', 'prob_o', 'met_o'],\n",
    "             axis=1)\n",
    "df = df.dropna(subset=['age'])\n",
    "df.loc[:, 'field_cd'] = df.loc[:, 'field_cd'].fillna(0)\n",
    "df = df.drop(['field'], axis=1)\n",
    "df = df.drop(['undergra'], axis=1)\n",
    "df.loc[:, 'mn_sat'] = df.loc[:, 'mn_sat'].str.replace(',', '').astype(np.float)\n",
    "df.loc[:, 'mn_sat'] = df.mn_sat.fillna(-999)\n",
    "df.loc[:, 'tuition'] = df.loc[:, 'tuition'].str.replace(',', '').astype(np.float)\n",
    "df.loc[:, 'tuition'] = df.tuition.fillna(-999)\n",
    "df = df.dropna(subset=['imprelig', 'imprace'])\n",
    "df = df.drop(['from', 'zipcode'], axis=1)\n",
    "df.loc[:, 'income'] = df.loc[:, 'income'].str.replace(',', '').astype(np.float)\n",
    "df.loc[:, 'income'] = df.loc[:, 'income'].fillna(-999)\n",
    "df = df.dropna(subset=['date'])\n",
    "df.loc[:, 'career_c'] = df.loc[:, 'career_c'].fillna(0)\n",
    "df = df.drop(['career'], axis=1)\n",
    "df = df.drop(['sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking', 'gaming',\n",
    "              'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts', 'music',\n",
    "              'shopping', 'yoga'], axis=1)\n",
    "df = df.drop(['expnum'], axis=1)\n",
    "feat = ['iid', 'wave', 'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']\n",
    "temp = df.drop_duplicates(subset=['iid', 'wave']).loc[:, feat]\n",
    "temp.loc[:, 'totalsum'] = temp.iloc[:, 2:].sum(axis=1)\n",
    "idx = ((temp.wave < 6) | (temp.wave > 9)) & (temp.totalsum < 99)\n",
    "idx = ((temp.wave >= 6) & (temp.wave <= 9))\n",
    "df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1',\n",
    "                                        'amb1_1', 'shar1_1']].sum(axis=1)\n",
    "df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']] = \\\n",
    "    (df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1',\n",
    "                'shar1_1']].T / df.loc[:, 'temp_totalsum'].T).T * 100\n",
    "feat = ['iid', 'wave', 'attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']\n",
    "temp = df.drop_duplicates(subset=['iid', 'wave']).loc[:, feat]\n",
    "temp.loc[:, 'totalsum'] = temp.iloc[:, 2:].sum(axis=1)\n",
    "idx = ((temp.wave < 6) | (temp.wave > 9)) & (temp.totalsum < 90) & (temp.totalsum != 0)\n",
    "idx = ((temp.wave >= 6) & (temp.wave <= 9))\n",
    "df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1',\n",
    "                                        'amb2_1', 'shar2_1']].sum(axis=1)\n",
    "df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']] = \\\n",
    "    (df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1',\n",
    "                'shar2_1']].T / df.loc[:, 'temp_totalsum'].T).T * 100\n",
    "df = df.drop(['temp_totalsum'], axis=1)\n",
    "for i in [4, 5]:\n",
    "    feat = ['attr{}_1'.format(i), 'sinc{}_1'.format(i),\n",
    "            'intel{}_1'.format(i), 'fun{}_1'.format(i),\n",
    "            'amb{}_1'.format(i), 'shar{}_1'.format(i)]\n",
    "\n",
    "    if i != 4:\n",
    "        feat.remove('shar{}_1'.format(i))\n",
    "\n",
    "    df = df.drop(feat, axis=1)\n",
    "df = df.drop(['wave'], axis=1)\n",
    "df_male = df.query('gender == 1').drop_duplicates(subset=['iid', 'pid']) \\\n",
    "    .drop(['gender'], axis=1) \\\n",
    "    .dropna()\n",
    "df_female = df.query('gender == 0').drop_duplicates(subset=['iid']) \\\n",
    "    .drop(['gender', 'match', 'int_corr', 'samerace'], axis=1) \\\n",
    "    .dropna()\n",
    "\n",
    "df_female.columns = df_female.columns + '_f'\n",
    "df_female = df_female.drop(['pid_f'], axis=1)\n",
    "df_pair = df_male.join(df_female.set_index('iid_f'),\n",
    "                       on='pid',\n",
    "                       how='inner')\n",
    "df_pair = df_pair.drop(['iid', 'pid'], axis=1)\n",
    "X = df_pair.iloc[:, 1:].values\n",
    "y = df_pair.iloc[:, 0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf = MyDecisionTreeClassifier(min_samples_split=2)\n",
    "clf = DecisionTreeClassifier(min_samples_split=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка скорости работы на Speed Dating Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 113 ms, sys: 377 µs, total: 113 ms\n",
      "Wall time: 114 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.33 s, sys: 0 ns, total: 1.33 s\n",
      "Wall time: 1.33 s\n"
     ]
    }
   ],
   "source": [
    "%time my_clf.fit(X_train, y_train.reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка качества работы на Speed Dating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5321637426900585"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5611218154570756"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=my_clf.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_one():\n",
    "    global df, X, y\n",
    "    df = df.iloc[:, :97]\n",
    "    df = df.drop(['id'], axis=1)\n",
    "    df = df.drop(['idg'], axis=1)\n",
    "    df = df.drop(['condtn'], axis=1)\n",
    "    df = df.drop(['round'], axis=1)\n",
    "    df = df.drop(['position', 'positin1'], axis=1)\n",
    "    df = df.drop(['order'], axis=1)\n",
    "    df = df.drop(['partner'], axis=1)\n",
    "    df = df.drop(['age_o', 'race_o', 'pf_o_att',\n",
    "                  'pf_o_sin', 'pf_o_int',\n",
    "                  'pf_o_fun', 'pf_o_amb', 'pf_o_sha',\n",
    "                  'dec_o', 'attr_o', 'sinc_o', 'intel_o', 'fun_o',\n",
    "                  'amb_o', 'shar_o', 'like_o', 'prob_o', 'met_o'],\n",
    "                 axis=1)\n",
    "    df = df.dropna(subset=['age'])\n",
    "    df.loc[:, 'field_cd'] = df.loc[:, 'field_cd'].fillna(0)\n",
    "    df = df.drop(['field'], axis=1)\n",
    "    df = df.drop(['undergra'], axis=1)\n",
    "    df.loc[:, 'mn_sat'] = df.loc[:, 'mn_sat'].str.replace(',', '').astype(np.float)\n",
    "    df.loc[:, 'mn_sat'] = df.mn_sat.fillna(-999)\n",
    "    df.loc[:, 'tuition'] = df.loc[:, 'tuition'].str.replace(',', '').astype(np.float)\n",
    "    df.loc[:, 'tuition'] = df.tuition.fillna(-999)\n",
    "    df = df.dropna(subset=['imprelig', 'imprace'])\n",
    "    df = df.drop(['from', 'zipcode'], axis=1)\n",
    "    df.loc[:, 'income'] = df.loc[:, 'income'].str.replace(',', '').astype(np.float)\n",
    "    df.loc[:, 'income'] = df.loc[:, 'income'].fillna(-999)\n",
    "    df = df.dropna(subset=['date'])\n",
    "    df.loc[:, 'career_c'] = df.loc[:, 'career_c'].fillna(0)\n",
    "    df = df.drop(['career'], axis=1)\n",
    "    df = df.drop(['sports', 'tvsports', 'exercise', 'dining', 'museums', 'art', 'hiking',\n",
    "                  'gaming', 'clubbing', 'reading', 'tv', 'theater', 'movies', 'concerts',\n",
    "                  'music', 'shopping', 'yoga'], axis=1)\n",
    "    df = df.drop(['expnum'], axis=1)\n",
    "    feat = ['iid', 'wave', 'attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']\n",
    "    temp = df.drop_duplicates(subset=['iid', 'wave']).loc[:, feat]\n",
    "    temp.loc[:, 'totalsum'] = temp.iloc[:, 2:].sum(axis=1)\n",
    "    idx = ((temp.wave < 6) | (temp.wave > 9)) & (temp.totalsum < 99)\n",
    "    idx = ((temp.wave >= 6) & (temp.wave <= 9))\n",
    "    df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1',\n",
    "                                            'fun1_1', 'amb1_1', 'shar1_1']].sum(axis=1)\n",
    "    df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1', 'amb1_1', 'shar1_1']] = \\\n",
    "        (df.loc[:, ['attr1_1', 'sinc1_1', 'intel1_1', 'fun1_1',\n",
    "                    'amb1_1', 'shar1_1']].T / df.loc[:, 'temp_totalsum'].T).T * 100\n",
    "    feat = ['iid', 'wave', 'attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']\n",
    "    temp = df.drop_duplicates(subset=['iid', 'wave']).loc[:, feat]\n",
    "    temp.loc[:, 'totalsum'] = temp.iloc[:, 2:].sum(axis=1)\n",
    "    idx = ((temp.wave < 6) | (temp.wave > 9)) & (temp.totalsum < 90) & (temp.totalsum != 0)\n",
    "    idx = ((temp.wave >= 6) & (temp.wave <= 9))\n",
    "    df.loc[:, 'temp_totalsum'] = df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1',\n",
    "                                            'fun2_1', 'amb2_1', 'shar2_1']].sum(axis=1)\n",
    "    df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1', 'fun2_1', 'amb2_1', 'shar2_1']] = \\\n",
    "        (df.loc[:, ['attr2_1', 'sinc2_1', 'intel2_1',\n",
    "                    'fun2_1', 'amb2_1', 'shar2_1']].T / df.loc[:, 'temp_totalsum'].T).T * 100\n",
    "    df = df.drop(['temp_totalsum'], axis=1)\n",
    "    for i in [4, 5]:\n",
    "        feat = ['attr{}_1'.format(i), 'sinc{}_1'.format(i),\n",
    "                'intel{}_1'.format(i), 'fun{}_1'.format(i),\n",
    "                'amb{}_1'.format(i), 'shar{}_1'.format(i)]\n",
    "\n",
    "        if i != 4:\n",
    "            feat.remove('shar{}_1'.format(i))\n",
    "\n",
    "        df = df.drop(feat, axis=1)\n",
    "    df = df.drop(['wave'], axis=1)\n",
    "    df_male = df.query('gender == 1').drop_duplicates(subset=['iid', 'pid']) \\\n",
    "        .drop(['gender'], axis=1) \\\n",
    "        .dropna()\n",
    "    df_female = df.query('gender == 0').drop_duplicates(subset=['iid']) \\\n",
    "        .drop(['gender', 'match', 'int_corr', 'samerace'], axis=1) \\\n",
    "        .dropna()\n",
    "\n",
    "    df_female.columns = df_female.columns + '_f'\n",
    "    df_female = df_female.drop(['pid_f'], axis=1)\n",
    "    df_pair = df_male.join(df_female.set_index('iid_f'),\n",
    "                           on='pid',\n",
    "                           how='inner')\n",
    "    df_pair = df_pair.drop(['iid', 'pid'], axis=1)\n",
    "    X = df_pair.iloc[:, 1:].values\n",
    "    y = df_pair.iloc[:, 0].values\n",
    "\n",
    "\n",
    "def func_two():\n",
    "    global X, y, X_train, X_test, y_train, y_test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(np.array(X), np.array(y), test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "df = pd.read_csv('Speed Dating Data.csv',\n",
    "                 encoding='cp1251')\n",
    "X = np.empty(0)\n",
    "y = np.empty(0)\n",
    "X_train = np.empty(0)\n",
    "X_test = np.empty(0)\n",
    "y_train = np.empty(0)\n",
    "y_test = np.empty(0)\n",
    "pipeline1 = Pipeline(\n",
    "    [('first', func_one()), ('second', func_two()), ('third', MyDecisionTreeClassifier(min_samples_split=2))])\n",
    "df = pd.read_csv('Speed Dating Data.csv',\n",
    "                 encoding='cp1251')\n",
    "pipeline2 = Pipeline(\n",
    "    [('first', func_one()), ('second', func_two()), ('third', DecisionTreeClassifier(min_samples_split=2))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_clf_pipe = pipeline1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipe = pipeline2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5186003484758671"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=my_clf_pipe.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5294117647058824"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_pred=clf_pipe.predict(X_test), y_true=y_test, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df_pair.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int_corr      0.053160\n",
       "field_cd      0.042527\n",
       "imprace_f     0.041369\n",
       "age_f         0.036940\n",
       "age           0.036593\n",
       "samerace      0.036132\n",
       "race          0.034012\n",
       "fun1_1_f      0.031843\n",
       "go_out_f      0.030052\n",
       "intel1_1_f    0.029736\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(index=names, data=my_clf.feature_importances()).sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int_corr      0.068601\n",
       "amb1_1_f      0.048948\n",
       "age           0.036049\n",
       "age_f         0.034163\n",
       "career_c_f    0.030970\n",
       "sinc1_1       0.029565\n",
       "attr3_1       0.028514\n",
       "attr3_1_f     0.024953\n",
       "shar1_1       0.022555\n",
       "attr1_1_f     0.021998\n",
       "dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(index=names, data=clf.feature_importances_).sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'n_estimators': range(10, 21),\n",
    "    'max_features': ['auto', 'log2', None]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_cv = GridSearchCV(classifier, params_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_estimators': [10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 'max_features': ['auto', 'log2', None], 'criterion': ['gini', 'entropy']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini', 'max_features': 'auto', 'n_estimators': 16}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8405112531258683"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
