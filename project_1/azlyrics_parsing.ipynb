{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests      # Библиотека для отправки запросов\n",
    "import numpy as np   # Библиотека для матриц, векторов и линала\n",
    "import pandas as pd  # Библиотека для табличек\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import socks\n",
    "import socket\n",
    "socks.set_default_proxy(socks.SOCKS5, \"localhost\", 9150)\n",
    "socket.socket = socks.socksocket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Получаем ссылки на каждую букву алфавита"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_link = 'https://www.azlyrics.com/'\n",
    "response = requests.get(page_link, headers={'User-Agent': UserAgent().chrome})\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html,'lxml')\n",
    "objs = soup.findAll('a', attrs = {'class':'btn btn-menu'})\n",
    "links=[str(link.attrs['href'])[2:] for link in objs]\n",
    "# в итоге в links ссылки на каждую букву"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Получаем ссылки каждую группу (важно, что могут быть дубликаты)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_band_links=[]\n",
    "for i,letter_page in enumerate(links):\n",
    "    if(i%1000==0):\n",
    "        print i\n",
    "    try:\n",
    "        response = requests.get(page_link, headers={'User-Agent': UserAgent().chrome})\n",
    "    except:\n",
    "        print i,'Сейчас исключение в requestе, переподключай интернет'\n",
    "        time.sleep(30)\n",
    "        response = requests.get(page_link, headers={'User-Agent': UserAgent().chrome})\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    objs = soup.findAll('a')\n",
    "    band_links=[str(link.attrs['href']) for link in objs]\n",
    "    for j,band_link in enumerate(band_links):\n",
    "        band_links[j]='https://www.azlyrics.com/'+band_link\n",
    "    band_links=band_links[28:]\n",
    "    band_links=band_links[:-7]\n",
    "    total_band_links.append(band_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_total_band_links = [item for sublist in total_band_links for item in sublist] #получаем 1d list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_total_band_links=np.array(flat_total_band_links)\n",
    "unique_links=np.unique(np_total_band_links) #получаем уникальные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"band_links.txt\", unique_links,fmt='%s') #сохраняем ссылки на каждую группу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь получим ссылки на каждую песню"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_links=np.loadtxt('band_links.txt',dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_song_links_list=[]\n",
    "for i,page_link in enumerate(unique_links):\n",
    "    if((i+1)%100==0):\n",
    "        print i\n",
    "    ch=-1\n",
    "    while (True):\n",
    "        try:\n",
    "            response = requests.get(page_link, headers={'User-Agent': UserAgent().chrome})\n",
    "            break\n",
    "        except:\n",
    "            if not i==ch:\n",
    "                print 'Sleeping now at i=',i\n",
    "            time.sleep(1)\n",
    "            ch=i\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    obj = soup.find('div',attrs={'id':'listAlbum'})\n",
    "    try:\n",
    "        objs = obj.findAll('a')\n",
    "    except:\n",
    "        print page_link # здесь оказываются те, у кого нет альбомов, там другая разметка страницы\n",
    "        continue\n",
    "    help_list=[]\n",
    "    for item in objs:\n",
    "        if (not type(item.attrs.get('href'))==type(None)):\n",
    "            help_list.append('https://www.azlyrics.com/'+str(item.attrs.get('href'))[3:])\n",
    "    total_song_links_list.append(help_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_total_song_links_list = [item for sublist in total_song_links_list for item in sublist] #получаем 1d list\n",
    "np_song_links=np.array(flat_total_song_links_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_links_unique = np.unique(song_links) #получаем уникальные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_links_unique = np.array([x for x in song_links_unique if(str(x).find('google.com')==-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_links=[]\n",
    "for line in song_links_unique:\n",
    "    if(str(line).find('/ly')==-1):\n",
    "        new_links.append(line)\n",
    "    else:\n",
    "        new_links.append('https://www.azlyrics.com'+line[str(line).find('/ly'):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_links_unique=np.array(new_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"song_links_unique.txt\", song_links_unique,fmt='%s') #сохраняем ссылки на каждую песню."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь непосредственно получаем текст песни, Название, Исполнителя, год и Альбом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['Artist','Song_name','Album','Year','Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_links=np.loadtxt('song_links_unique.txt',dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(493437,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_links.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,page_link in enumerate(song_links[250000::10]):\n",
    "    if(i%1000==0):\n",
    "        print i\n",
    "    ch=-1\n",
    "    counter=0\n",
    "    while (True):\n",
    "        try:\n",
    "            response = requests.get(page_link, headers={'User-Agent': UserAgent().chrome})\n",
    "            break\n",
    "        except:\n",
    "            if not i==ch:\n",
    "                print 'Sleeping now at i=',i\n",
    "            time.sleep(1)\n",
    "            ch=i\n",
    "            counter+=1\n",
    "            if counter==60:\n",
    "                break\n",
    "    try:\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html,'lxml')\n",
    "        obj=soup.find('div',{'class':'col-xs-12 col-lg-8 text-center'})\n",
    "        band_song = obj.findAll('b')\n",
    "        band_name=str(band_song[0].text)[:-7]\n",
    "        song_name=str(band_song[1].text)[1:-1]\n",
    "        obj=soup.find('div',{'class':'col-xs-12 col-lg-8 text-center'})\n",
    "        song_text=str(obj.findAll('div')[6].text.encode('utf8')).replace('\\n' ,' ').replace('\\42' ,'').replace('\\r' ,'')\n",
    "        alb=soup.find('a',{'data-toggle':'collapse'})\n",
    "        if(type(alb)==type(None)):\n",
    "            alb_name=np.nan\n",
    "            year=np.nan\n",
    "        else:\n",
    "            alb_name=str(alb.text)[1:-8]\n",
    "            year=str(alb.text)[-5:-1]\n",
    "        data_row = {\"Artist\":band_name, \"Song_name\":song_name,\"Album\":alb_name, \"Year\":year,\"Text\":song_text}\n",
    "        df = df.append(data_row, ignore_index=True)\n",
    "    except:\n",
    "        print 'Unknown exception on page:', page_link\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkIP():\n",
    "    ip = requests.get('http://checkip.dyndns.org').content\n",
    "    soup = BeautifulSoup(ip, 'html.parser')\n",
    "    print(soup.find('body').text)\n",
    "\n",
    "checkIP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
